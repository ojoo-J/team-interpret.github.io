<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>1.1 Neurons in Neuroscience and Deep Learning | SAIL Team Interpretability</title> <meta name="author" content="SAIL Team Interpretability"> <meta name="description" content="📙 Chapter 1. Neuron &lt;br&gt; &lt;em&gt; How can we compare neurons in neuroscience and deep-learning &lt;/em&gt; "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://team-interpret.github.io/curriculum/1_neuron_comparison/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "1.1 Neurons in Neuroscience and Deep Learning",
      "description": "📙 Chapter 1. Neuron <br> <em> How can we compare neurons in neuroscience and deep-learning </em> ",
      "published": "August 7, 2023",
      "authors": [
        {
          "author": "Bumjin Park",
          "authorURL": "",
          "affiliations": [
            {
              "name": "KAIST",
              "url": ""
            }
          ]
        },
        {
          "author": "Youngju Joung",
          "authorURL": "",
          "affiliations": [
            {
              "name": "KAIST",
              "url": ""
            }
          ]
        },
        {
          "author": "Enver Menadjiev",
          "authorURL": "",
          "affiliations": [
            {
              "name": "KAIST",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">SAIL </span>Team Interpretability</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/curriculum/">Curriculum</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>1.1 Neurons in Neuroscience and Deep Learning</h1> <p>📙 Chapter 1. Neuron <br> <em> How can we compare neurons in neuroscience and deep-learning </em> </p> </d-title> <d-byline></d-byline> <d-article> <h2 id="introduction">Introduction</h2> <p>There are several attempts to understand human in the fields of biology, neuroscience, ethology, and philosophy. Especially, neuroscience interprets human with neurons in a brain. Neurons in a brain process massive information and the intelligence increases as the number of neurons increases. However, understanding the general role of a single neuron is hardly discovered as we can not test the neuron separately excluding other neurons. However, unlike the neurons in neuroscience, neurons in deep learning could be tested easily as we can debug the network. Recent studies have shown the role of individual neurons in both vision and NLP domains and we can understand the interaction between neurons well. If the neurons in our brain compute and process the signals with the neurons in deep learning, we can further understand neurons in both field. To do this, we should know how similar neurons in neuroscience and deep learning are.</p> <p>The reason why I wrote this post, even though I studied deep learning, is that understanding the neurons in neuroscience can further helps to reveal the mechanisms of neurons in deep learning. To components are similar to each other, but have distinct difference such as learning processes. We study the question <strong>How much neurons are similar in both fields?</strong> Before answering this question we should answer this question. <strong>What is a neuron in deep learning?</strong></p> <h2 id="neuroscience">Neuroscience</h2> <p>Human body has cell and there are cells called <strong>neuron</strong> whose role is the transmission of signals. Neurons communicate each other by chemical materials called <strong>synapse</strong> which is the connection part between two neurons. In human brain, there are 860B number of neurons and a single neuron has 7,000 synapses. Therefore, human brain has 6 trillion synapses.</p> \[\begin{gather} \text{Number of neurons} \times \text{Number of synapses per a neuron} \approx \text{Total number of synapses} \\ \Rightarrow 860 \text{B} \times 7,000 \approx 6 \text{trillion} \end{gather}\] <p>The number of synapses is the number of connections and the neurons has information in it. When a single neuron is fired, we can consider the successively fired neurons like a chain. Consider the following example.</p> <ol> <li>Eyes get light signals (<strong>Dog image</strong> 🦮)</li> <li>neurons in eyes are fired and signal is transmitted to the neurons in the brain (<strong>Neurons related to vision</strong> 👀)</li> <li>Several neurons in a brain are affected by the vision neurons <ul> <li>Emergence of vocabulary “dog” (<strong>Dog Neuron</strong> 🐶)</li> <li>Emergence of vocabulary “cat” by “dog” (<strong>Cat Neuron</strong> 😸)</li> <li>Prediction of the dog movement (<strong>Future Prediction Neuron</strong> ⌛️)</li> <li>Emergence of traumas related to a dog (<strong>Long-term Memory Neuron</strong> 🐾)</li> <li>Trauma makes me feel bad (<strong>Emotion Neuron</strong> 😢)</li> </ul> </li> </ol> <p>Note that even though we only observed the dog image, other neurons related to the <strong>dog</strong> are fired. Understanding the circuits of neurons are like finding such consequent events of neurons.</p> <hr> <h3 id="neuron">Neuron</h3> <blockquote> <p>These videos can help your understanding of neurons</p> </blockquote> <div> <iframe width="340" height="315" src="https://www.youtube.com/embed/SczOfOXY17U"> </iframe> <iframe width="340" height="315" src="https://www.youtube.com/embed/GIGqp6_PG6k"> </iframe> </div> <h4 id="basic-structure">Basic Structure</h4> <p>Neurons communicate with each other with chemical materials such as Na+ and Cl-, and process the signal in terms of electricity. To understand this communication process, lets see what components a neuron has.</p> <figure style="text-align:center; display:block;"> <img src="/assets/img/neurons_as_neurons/neuron.png" style="width:100%"> <figcaption> Figure. Basic structure of a neuron. The dendrite receives input chemical, axon transfers the electricity, and axon terminal outputs chemical material again. </figcaption> </figure> <ul> <li> <code class="language-plaintext highlighter-rouge">dendrites</code> : Obtains chemical materials from other neurons (receiver)</li> <li> <code class="language-plaintext highlighter-rouge">SOMA</code> (Nuclear) : The nuclear of a neuron</li> <li> <code class="language-plaintext highlighter-rouge">Axon</code> : connects <code class="language-plaintext highlighter-rouge">dendrites</code> and <code class="language-plaintext highlighter-rouge">axon terminal</code> and transfers the electrical signal.</li> <li> <code class="language-plaintext highlighter-rouge">Axon terminals</code> : transmits chemical materials to other neurons (publisher)</li> </ul> <h4 id="communication">Communication</h4> <p>A neuron is normally a negative state (-70mv) and if a <code class="language-plaintext highlighter-rouge">dendrite</code> receives negative ion, the state is still negative. On the other hand, if <code class="language-plaintext highlighter-rouge">dendrite</code> receives positive ions such as $Na^{+}$, the dendrite part becomes positive state. Then, the electricity flows from <code class="language-plaintext highlighter-rouge">dendrite</code> to <code class="language-plaintext highlighter-rouge">axon terminal</code> in a neuron to make <code class="language-plaintext highlighter-rouge">Neutral Charge Equilibrium</code> state. The pipe for the transmission is called axon and the leaf part is <code class="language-plaintext highlighter-rouge">axon terminal</code>. When the <code class="language-plaintext highlighter-rouge">axon terminal</code> becomes positive state, neurotransmitters spread.</p> <figure style=""> <img src="/assets/img/neurons_as_neurons/neuron_state.png" style="width:100%"> <figcaption> How a neuron transmits electrical signal from dendrites to axon terminals. Neuron is basically negative state (-), the activated part are relatively positive state (+). </figcaption> </figure> <p>In summary, a successive propagation of chemical makes the communication.</p> <h3 id="synapse">Synapse</h3> <p>Synapse is the connected parts of <code class="language-plaintext highlighter-rouge">dendrite</code> and <code class="language-plaintext highlighter-rouge">axon terminal</code>. Synapse is the location where the communication between two neurons occur, and we call the sender and receiver as follows: <code class="language-plaintext highlighter-rouge">N: Neuron, [*] Other Neurons</code></p> <ul> <li> <strong>(Left) Presynaptic Neuron</strong> : a neuron transmits signal with <code class="language-plaintext highlighter-rouge">axon terminal</code> $(N \rightarrow [*])$</li> <li> <strong>(Right) Postynaptic Neuron</strong> : a neuron receives with <code class="language-plaintext highlighter-rouge">dendrite</code> \(([*] \rightarrow N)\)</li> </ul> <figure style="text-align:center; display:block;width:100;"> <img src="/assets/img/neurons_as_neurons/synapse.png" style="width:100%"> <figcaption> Figure. Two neurons are connected and the synapse is the connection. The sender gives signal with the axon terminal and the receiver gets signal wit the dendrite. </figcaption> </figure> <h4 id="aggregation-of-received-signal">Aggregation of Received signal</h4> <p>The electrical signal flows in a neuron is either (+) or (-), but the received chemicals at dendrites are combined. Therefore, the aggregated (summed) electrical signal is transferred along <code class="language-plaintext highlighter-rouge">Axon</code>. In other words, the ions $T_1, T_2, \cdots$ at dendrites are summed and generate the electrical signal.<br> \(\text{Electrical Signal} = T_1 + T_2 + T_3 + T_4 + \cdots\)</p> <figure style="text-align:center; display:block;width:100;"> <img src="/assets/img/neurons_as_neurons/transmission.png" style="width:100%"> <figcaption style="text-align:left"> Figure. (Up) Several neurons transmit chemicals to the postsynaptic neuron. (Down) as axon becomes positive state, axon terminals spread chemicals to the connected neurons. </figcaption> </figure> <p>Recall that there is only one electrical signal, but the several axon terminals transmit different kinds of neurotransmitters. Neurotransmitters are categorized by whether the chemical further excites the connected neurons. The excitatory neuron spreads positive ions, while the inhibitory neuron spreads negative ions. See <strong>Thread: Circuits</strong> <d-cite key="cammarata2020thread"></d-cite> [<a href="https://distill.pub/2020/circuits/" rel="external nofollow noopener" target="_blank">post</a>] for the detailed examples in CNN.</p> <p><strong>Excitatory and Inhibitory Neurotransmitters</strong></p> <ul> <li>🔥 Excitatory Neurotransmitters <ul> <li>Na+</li> <li>Acetylcholine</li> <li>Noradrenaline</li> <li>Glutamate</li> </ul> </li> <li>❄️ Inhibitory Neurotransmitters <ul> <li>Glycine</li> <li>GABA (gamma-aminobutyric acid)</li> <li>Cl-</li> </ul> </li> </ul> <figure style=""> <img src="/assets/img/neurons_as_neurons/excitation_and_inhibition.png" style="width:120%"> <figcaption> Excitatory neuron make the next neuron activated, while the inhibitory neuron prevents the activation of the connected neuron. </figcaption> </figure> <hr> <h2 id="deep-learning">Deep Learning</h2> <h3 id="neuron--synapse">Neuron &amp; Synapse</h3> <p>In the neuroscience section, we introduced the electrical signal made by the multiple chemical signals from dendrites. This operation is same with the dot product which results in a single scalar.</p> <figure style="text-align:center;"> <img src="/assets/img/neurons_as_neurons/neuron_as_neuron.png" style="width:100%"> <figcaption> Figure. The similarity between a neuron in neuroscience and a neuron in deep learning. </figcaption> </figure> <p>In linear layer, we can interpret each row as a neuron</p> <h4 id="-neuron-in-deep-learning-motivated-by-the-neuroscience">🚀 Neuron in deep-learning motivated by the neuroscience.**</h4> <p><strong>Neuron</strong> : collection of weights which outputs a single scalar. <strong>Synapse</strong> : each weight in the neuron</p> <ul> <li>Neuron in neuroscience <ul> <li>(In) <strong>the amount of $K$ chemicals</strong>: $a_1, a_2, a_3, \cdots, a_k $</li> <li>(Weight) <strong>K dendrites</strong>: $w_1, w_2, w_3, \cdots, w_k $ (positive or negative)</li> <li>(Out) <strong>How much activation occurs</strong> : $\sum_k w_k \cdot a_k$</li> </ul> </li> <li>Neuron in deep learning <ul> <li>(In) <strong>Neuron Input</strong> : $a_1, a_2, a_3, \cdots, a_k $</li> <li>(Weight) <strong>Neuron Interpretation</strong> : $w_1, w_2, w_3, \cdots, w_k $</li> <li>(Out) <strong>Neuron Output</strong> : $\sum_k w_k \cdot a_k$</li> </ul> </li> </ul> <p>In the linear layer, we have weights and biases, $W\in \mathbb{R}^{N\times K}$ and $\mathbf{b}\in \mathbb{R}^{N}$. When we interpret this (1) there are <code class="language-plaintext highlighter-rouge">N</code> neurons, (2) each neuron accumulate $K$ signals, and (3) output is linear combined. As all $N$ neurons output a single scalar, the dimension of output is $N$.</p> \[\begin{gather} \mathbf{y} = W\mathbf{x} + \mathbf{b} \\ \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_N \end{bmatrix} = \begin{bmatrix} w_{11} &amp; w_{12}&amp; \cdots &amp; w_{1K} \\ w_{21} &amp; w_{22}&amp; \cdots &amp; w_{2K} \\ \vdots \\ w_{N1} &amp; w_{N2}&amp; \cdots &amp; w_{NK} \\ \end{bmatrix} \cdot \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_K \end{bmatrix} + \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_N \end{bmatrix} \end{gather}\] <ul> <li>⭐️ Number of Neurons : $N$ (Dimension of Output)</li> <li>⭐️ Number of Synapses : $N\times K$ (Number of Weights)</li> </ul> <h2 id="comparison">Comparison</h2> <p>In the previous section, we found that the dot-product could be interpreted as the computation of a neuroscience neuron. In this section, we compare how communications of two neurons are similar. In deep learning, we consider two message passing paths, <code class="language-plaintext highlighter-rouge">forward</code> and <code class="language-plaintext highlighter-rouge">backward</code>. However, the neurons in neuroscience only have <code class="language-plaintext highlighter-rouge">forward</code> pass to communicate with each other. In addition, the learning paradigm is different that the deep learning neuron learns from the back-propagation of signals, which is not true in neuroscience!</p> <h3 id="forward">Forward</h3> <p>The figure below shows the major difference in communication. Note that the neurons in neuroscience can form a circular message passing as they are located in the 3D physical system, while the deep learning has only a single forward pass and each neuron process information only one time <d-footnote> If we apply the parameter sharing, a single neuron can process information multiple times. </d-footnote>. Note that even RNN which is known to process multiple inputs also computes information only one time.</p> <p>In addition, neurons in neuroscience communicate in asynchronous manner, but the computation of neurons in deep learning is synchronous for every layer. Therefore, humans are more stochastic than neurons in deep learning.</p> <figure style="text-align:center; display:block;width:100;"> <img src="/assets/img/neurons_as_neurons/communication.png" style="width:100%"> <figcaption> Figure. How neurons communicate with each other. For the neurons in the neuroscience, they could form a circular communication, while the deep learning neurons from only one-directional signal processing. </figcaption> </figure> <h3 id="backward">Backward</h3> <p>The backward is a natural way of training a deep neural network. The weights in neurons are updated to minimize an objective and back-propagation signal passes in backward. Unlike this, neurons in neuroscience can receive and send information with only two fixed components, dendrites and axon terminals. Therefore, we can not think the neurons in neuroscience use back-propagation to learn. Recent work of Geoffrey Hinton [<a href="https://scholar.google.com/citations?user=JicYPdAAAAAJ&amp;hl=ko&amp;oi=ao" rel="external nofollow noopener" target="_blank">scholar</a>] is inspired from this observation, and he proposed forward-forward algorithm <d-cite key="hinton2022forward"></d-cite>.</p> <h2 id="type-of-information">Type of Information</h2> <p>The most significant unfairness of deep learning compared to humans is the difference in input. Human eyes receive information as input every moment, and about 200 images are received in high resolution per second. But the deep learning models take an input of the level of an image. Although recent models are designed to take more inputs in order to approximate the amount of information a human receives, deep learning still has the disadvantage of reacting only when input is given.</p> <p>In addition, unlike humans, deep learnings only work when an input is given: <em>What can deep learning do if there is no input?</em></p> <h3 id="absence-of-state">Absence of State</h3> <p>In humans, neurons can respond and process information without being given input (e.g., you can recall a picture in your mind and draw conclusions or create new ideas from that picture). This process is considered to retrieve information which is stored in the brain’s long-term memory, but the deep learning parameters are configured to perform operations on given inputs and there is no memory.</p> <h3 id="state-of-creature">State of Creature</h3> <p>Another difference is the type of input. While humans receive information through sensory organs, deep learning models have a fixed input space, and visual, linguistic, or audio information comes in through the input layer. Therefore, the information processed by deep learning is only the input and the output calculated by the internal neurons. However, for humans, there is additional information in addition to the information received as input. It is the state of the body or cognition.</p> <h3 id="state-of-body">State of body</h3> <blockquote> <p>Do deep learning models change calculation results just because they are tired? No.</p> </blockquote> <p>In humans, even given the same input, the result can be changed by current hormonal conditions. For example, If you feel sad, you can have different interpretations of the same picture. This suggests that the processing of neurons can be different depending on the body’s states.</p> <h3 id="state-of-cognition">State of cognition</h3> <blockquote> <p>Does deep learning keep thinking about information it has seen before? No.</p> </blockquote> <p>Cognitive contrast is a principle in which the next information is distorted and interpreted by the previously seen information. For example, a tiger next to an elephant is perceived as small because the elephant is large. However we cannot be sure that deep learning neurons analyze things based on this principle of cognitive contrast. They are more likely to interpret simply depending on the information currently available.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/all.bib"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"team-interpret/team-interpret","data-repo-id":"R_kgDOKCB5Vw","data-category":"Q&A","data-category-id":"DIC_kwDOKCB5V84CYQ_P","data-mapping":"title","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 SAIL Team Interpretability. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>