

# Timeseries, Discretization
@article{chaudhari2014discretization,
  title={Discretization of temporal data: a survey},
  author={Chaudhari, P and Rana, Dipti P and Mehta, Rupa G and Mistry, Narendra J and Raghuwanshi, Mukesh M},
  journal={arXiv preprint arXiv:1402.4283},
  year={2014}
}

# Timeseries, Discretization, Symbolic
@inproceedings{lin2003symbolic,
  title={A symbolic representation of time series, with implications for streaming algorithms},
  author={Lin, Jessica and Keogh, Eamonn and Lonardi, Stefano and Chiu, Bill},
  booktitle={Proceedings of the 8th ACM SIGMOD workshop on Research issues in data mining and knowledge discovery},
  pages={2--11},
  year={2003}
}

# Timeseries, Discretization
@article{rabanser2020effectiveness,
  title={The effectiveness of discretization in forecasting: An empirical study on neural time series models},
  author={Rabanser, Stephan and Januschowski, Tim and Flunkert, Valentin and Salinas, David and Gasthaus, Jan},
  journal={arXiv preprint arXiv:2005.10111},
  year={2020}
}



# Watermark
@article{fernandez2023stable,
  title={The stable signature: Rooting watermarks in latent diffusion models},
  author={Fernandez, Pierre and Couairon, Guillaume and J{\'e}gou, Herv{\'e} and Douze, Matthijs and Furon, Teddy},
  journal={arXiv preprint arXiv:2303.15435},
  year={2023}
}

# Watermark
@article{wen2023tree,
  title={Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust},
  author={Wen, Yuxin and Kirchenbauer, John and Geiping, Jonas and Goldstein, Tom},
  journal={arXiv preprint arXiv:2305.20030},
  year={2023}
}


# BERT 
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}


# GPT3
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

# WebGPT
@article{nakano2021webgpt,
  title={Webgpt: Browser-assisted question-answering with human feedback},
  author={Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
  journal={arXiv preprint arXiv:2112.09332},
  year={2021}
}

# LoRA
@inproceedings{hu2021lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

# Clip-Dissect
@inproceedings{oikarinen2022clip,
  title={CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks},
  author={Oikarinen, Tuomas and Weng, Tsui-Wei},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

# Parameter Efficient
@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}

# Thread:Circuits
@article{cammarata2020thread,
  author = {Cammarata, Nick and Carter, Shan and Goh, Gabriel and Olah, Chris and Petrov, Michael and Schubert, Ludwig and Voss, Chelsea and Egan, Ben and Lim, Swee Kiat},
  title = {Thread: Circuits},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/circuits},
  doi = {10.23915/distill.00024}
}

# Curve Circuits
@article{cammarata2021curve,
  author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Voss, Chelsea and Schubert, Ludwig and Olah, Chris},
  title = {Curve Circuits},
  journal = {Distill},
  year = {2021},
  note = {https://distill.pub/2020/circuits/curve-circuits},
  doi = {10.23915/distill.00024.006}
}

# Branch Specialization
@article{voss2021branch,
  author = {Voss, Chelsea and Goh, Gabriel and Cammarata, Nick and Petrov, Michael and Schubert, Ludwig and Olah, Chris},
  title = {Branch Specialization},
  journal = {Distill},
  year = {2021},
  note = {https://distill.pub/2020/circuits/branch-specialization},
  doi = {10.23915/distill.00024.008}
}

# MADDPG, MARL
@article{lowe2017multi,
  title={Multi-agent actor-critic for mixed cooperative-competitive environments},
  author={Lowe, Ryan and Wu, Yi I and Tamar, Aviv and Harb, Jean and Pieter Abbeel, OpenAI and Mordatch, Igor},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

# LToS, MARL 
@article{yi2022learning,
  title={Learning to Share in Networked Multi-Agent Reinforcement Learning},
  author={Yi, Yuxuan and Li, Ge and Wang, Yaowei and Lu, Zongqing},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={15119--15131},
  year={2022}
}

# IPPO, MARL
@article{de2020independent,
  title={Is independent learning all you need in the starcraft multi-agent challenge?},
  author={de Witt, Christian Schroeder and Gupta, Tarun and Makoviichuk, Denys and Makoviychuk, Viktor and Torr, Philip HS and Sun, Mingfei and Whiteson, Shimon},
  journal={arXiv preprint arXiv:2011.09533},
  year={2020}
}

# MAPPO, MARL 
@article{yu2022surprising,
  title={The surprising effectiveness of ppo in cooperative multi-agent games},
  author={Yu, Chao and Velu, Akash and Vinitsky, Eugene and Gao, Jiaxuan and Wang, Yu and Bayen, Alexandre and Wu, Yi},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24611--24624},
  year={2022}
} 

# TarMAC, MARL 
@inproceedings{das2019tarmac,
  title={Tarmac: Targeted multi-agent communication},
  author={Das, Abhishek and Gervet, Th{\'e}ophile and Romoff, Joshua and Batra, Dhruv and Parikh, Devi and Rabbat, Mike and Pineau, Joelle},
  booktitle={International Conference on Machine Learning},
  pages={1538--1546},
  year={2019},
  organization={PMLR}
}

# QMIX, MARL 
@inproceedings{rashid2018qmix,
  title={QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning},
  author={Rashid, Tabish and Samvelyan, Mikayel and Schroeder, Christian and Farquhar, Gregory and Foerster, Jakob and Whiteson, Shimon},
  booktitle={International Conference on Machine Learning},
  pages={4295--4304},
  year={2018},
  organization={PMLR}
}


# Mate, MARL
@inproceedings{pan2022mate,
  title     = {{MATE}: Benchmarking Multi-Agent Reinforcement Learning in Distributed Target Coverage Control},
  author    = {Xuehai Pan and Mickel Liu and Fangwei Zhong and Yaodong Yang and Song-Chun Zhu and Yizhou Wang},
  booktitle = {Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year      = {2022},
  url       = {https://openreview.net/forum?id=SyoUVEyzJbE}
}

# I2C, MARL 
@article{ding2020learning,
  title={Learning individually inferred communication for multi-agent cooperation},
  author={Ding, Ziluo and Huang, Tiejun and Lu, Zongqing},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={22069--22079},
  year={2020}
}


# N2G , Mechanistic Interpretability
@article{foote2023n2g,
  title={N2G: A Scalable Approach for Quantifying Interpretable Neuron Representations in Large Language Models},
  author={Foote, Alex and Nanda, Neel and Kran, Esben and Konstas, Ionnis and Barez, Fazl},
  journal={arXiv preprint arXiv:2304.12918},
  year={2023}
}


# Toy Model, Mechanistic Interpretability
@article{chughtai2023toy,
  title={A toy model of universality: Reverse engineering how networks learn group operations},
  author={Chughtai, Bilal and Chan, Lawrence and Nanda, Neel},
  journal={arXiv preprint arXiv:2302.03025},
  year={2023}
}

# Circuits, Mechanistic Interpretability
@article{olah2020zoom,
  title={Zoom in: An introduction to circuits},
  author={Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  journal={Distill},
  volume={5},
  number={3},
  pages={e00024--001},
  year={2020}
}

# Circuits, Mechanistic Interpretability
@article{cammarata2021curve,
  author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Voss, Chelsea and Schubert, Ludwig and Olah, Chris},
  title = {Curve Circuits},
  journal = {Distill},
  year = {2021},
  note = {https://distill.pub/2020/circuits/curve-circuits},
  doi = {10.23915/distill.00024.006}
}

# Transformer, Automata
@inproceedings{liu2022transformers,
  title={Transformers Learn Shortcuts to Automata},
  author={Liu, Bingbin and Ash, Jordan T and Goel, Surbhi and Krishnamurthy, Akshay and Zhang, Cyril},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}


# Chess, Mechanistic Interpretability 
@article{mcgrath2022acquisition,
  title={Acquisition of chess knowledge in alphazero},
  author={McGrath, Thomas and Kapishnikov, Andrei and Toma{\v{s}}ev, Nenad and Pearce, Adam and Wattenberg, Martin and Hassabis, Demis and Kim, Been and Paquet, Ulrich and Kramnik, Vladimir},
  journal={Proceedings of the National Academy of Sciences},
  volume={119},
  number={47},
  pages={e2206625119},
  year={2022},
  publisher={National Acad Sciences}
}


## World Representationn, Othello, Mechanistic Interpretability 
@inproceedings{li2022emergent,
  title={Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task},
  author={Li, Kenneth and Hopkins, Aspen K and Bau, David and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}


## diffusion, Concept
@article{patel2023conceptbed,
  title={ConceptBed: Evaluating Concept Learning Abilities of Text-to-Image Diffusion Models},
  author={Patel, Maitreya and Gokhale, Tejas and Baral, Chitta and Yang, Yezhou},
  journal={arXiv preprint arXiv:2306.04695},
  year={2023}
}




## Factual GPT, Interpretability
@article{meng2022locating,
  title={Locating and editing factual associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}

## OpenAI GPT Interpretability
@article{leike2023language,
  title={Language models can explain neurons in language models},
  author={Jan, Leike and Jeffrey, Wu and Steven, Bills and William, Saunders and Leo, Gao and Henk, Tillman and Daniel, Mossing},
  year={2022},
  journal={OpenAI},
}

## Activation Atlas, Interpretability
@article{carter2019activation,
  author = {Carter, Shan and Armstrong, Zan and Schubert, Ludwig and Johnson, Ian and Olah, Chris},
  title = {Activation Atlas},
  journal = {Distill},
  year = {2019},
  note = {https://distill.pub/2019/activation-atlas},
  doi = {10.23915/distill.00015}
}

## Circuits Thread, Interpretability
@article{schubert2021high-low,
  author = {Schubert, Ludwig and Voss, Chelsea and Cammarata, Nick and Goh, Gabriel and Olah, Chris},
  title = {High-Low Frequency Detectors},
  journal = {Distill},
  year = {2021},
  note = {https://distill.pub/2020/circuits/frequency-edges},
  doi = {10.23915/distill.00024.005}
}

## Circuits zoom in , Interpretability
@article{olah2020zoom,
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  title = {Zoom In: An Introduction to Circuits},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/circuits/zoom-in},
  doi = {10.23915/distill.00024.001}
}

## t-SNE, Interpretability
@article{wattenberg2016how,
  author = {Wattenberg, Martin and Viégas, Fernanda and Johnson, Ian},
  title = {How to Use t-SNE Effectively},
  journal = {Distill},
  year = {2016},
  url = {http://distill.pub/2016/misread-tsne},
  doi = {10.23915/distill.00002}
}

# Building Blocks, Interpretability 
@article{olah2018the,
  author = {Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
  title = {The Building Blocks of Interpretability},
  journal = {Distill},
  year = {2018},
  note = {https://distill.pub/2018/building-blocks},
  doi = {10.23915/distill.00010}
}

# deep dream, Interpretability 
@misc{mordvintsev2015inceptionism,
  author = {Alexander, Mordvintsev and Christopher, Olah and Mike, Tyka},
  title = {Inceptionism: Going Deeper into Neural Networks},
  year = {2016},
  url = {https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html},
}

# hinton, Interpretability
@article{hinton2022forward,
  title={The forward-forward algorithm: Some preliminary investigations},
  author={Hinton, Geoffrey},
  journal={arXiv preprint arXiv:2212.13345},
  year={2022}
}

# GPT4 
@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

# Neural Turing Machine
@misc{graves2014neural,
      title={Neural Turing Machines}, 
      author={Alex Graves and Greg Wayne and Ivo Danihelka},
      year={2014},
      eprint={1410.5401},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

# Blind, ICLR, RL
@article{wijmans2023emergence,
  title={Emergence of maps in the memories of blind navigation agents},
  author={Wijmans, Erik and Savva, Manolis and Essa, Irfan and Lee, Stefan and Morcos, Ari S and Batra, Dhruv},
  journal={arXiv preprint arXiv:2301.13261},
  year={2023}
}


# Checkerboard Artifact, CNN
@article{odena2016deconvolution,
  author = {Odena, Augustus and Dumoulin, Vincent and Olah, Chris},
  title = {Deconvolution and Checkerboard Artifacts},
  journal = {Distill},
  year = {2016},
  url = {http://distill.pub/2016/deconv-checkerboard},
  doi = {10.23915/distill.00003}
}


# Grokking 
@misc{nanda2023progress,
      title={Progress measures for grokking via mechanistic interpretability}, 
      author={Neel Nanda and Lawrence Chan and Tom Lieberum and Jess Smith and Jacob Steinhardt},
      year={2023},
      eprint={2301.05217},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


# Transformer Interpretability
@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}